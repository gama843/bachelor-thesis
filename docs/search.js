window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "d2r2", "modulename": "d2r2", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "d2r2.main", "modulename": "d2r2", "qualname": "main", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator", "modulename": "data_generator", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator", "modulename": "data_generator", "qualname": "DataGenerator", "kind": "class", "doc": "<p>A class to generate synthetic datasets for relational reasoning tasks.</p>\n\n<h2 id=\"attributes\">Attributes:</h2>\n\n<p>base_path : str\n    The base directory to save generated images and JSON files.\nimg_dim : int\n    The dimension (width and height) of the generated images.\nnum_images : int\n    The number of images to generate.\nmin_objects : int\n    Minimum number of objects to include in each image.\nmax_objects : int\n    Maximum number of objects to include in each image.\nvisualize_bboxes : bool\n    Whether to visualize bounding boxes of the objects.\nadd_noise : bool\n    Whether to add noise to the images.\nnoise_prob : float\n    Probability of noise presence in the images.\nfixed_colors : bool\n    Whether to use a fixed set of colors from the palette.\nfill_objects : bool\n    Whether the objects should be filled or not.\nshapes : dict\n    A dictionary of shape generation functions.\npalette : dict\n    A color palette for objects.\nreusable_palette_colors : bool\n    Whether colors from the palette can be reused.</p>\n"}, {"fullname": "data_generator.DataGenerator.__init__", "modulename": "data_generator", "qualname": "DataGenerator.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">base_path</span><span class=\"o\">=</span><span class=\"s1\">&#39;./data&#39;</span>,</span><span class=\"param\">\t<span class=\"n\">img_dim</span><span class=\"o\">=</span><span class=\"mi\">128</span>,</span><span class=\"param\">\t<span class=\"n\">num_images</span><span class=\"o\">=</span><span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">min_objects</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">max_objects</span><span class=\"o\">=</span><span class=\"mi\">6</span>,</span><span class=\"param\">\t<span class=\"n\">visualize_bboxes</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">add_noise</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">noise_prob</span><span class=\"o\">=</span><span class=\"mf\">0.05</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_colors</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">fill_objects</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">reusable_palette_colors</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">palette</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "data_generator.DataGenerator.base_path", "modulename": "data_generator", "qualname": "DataGenerator.base_path", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator.img_dim", "modulename": "data_generator", "qualname": "DataGenerator.img_dim", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator.num_images", "modulename": "data_generator", "qualname": "DataGenerator.num_images", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator.min_objects", "modulename": "data_generator", "qualname": "DataGenerator.min_objects", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator.max_objects", "modulename": "data_generator", "qualname": "DataGenerator.max_objects", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator.visualize_bboxes", "modulename": "data_generator", "qualname": "DataGenerator.visualize_bboxes", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator.add_noise", "modulename": "data_generator", "qualname": "DataGenerator.add_noise", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator.noise_prob", "modulename": "data_generator", "qualname": "DataGenerator.noise_prob", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator.fixed_colors", "modulename": "data_generator", "qualname": "DataGenerator.fixed_colors", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator.fill_objects", "modulename": "data_generator", "qualname": "DataGenerator.fill_objects", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator.reusable_palette_colors", "modulename": "data_generator", "qualname": "DataGenerator.reusable_palette_colors", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator.palette", "modulename": "data_generator", "qualname": "DataGenerator.palette", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator.shapes", "modulename": "data_generator", "qualname": "DataGenerator.shapes", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "data_generator.DataGenerator._prepare_directory", "modulename": "data_generator", "qualname": "DataGenerator._prepare_directory", "kind": "function", "doc": "<p>@public</p>\n\n<p>Prepare the base directory for saving images and JSON files.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._gen_rectangle", "modulename": "data_generator", "qualname": "DataGenerator._gen_rectangle", "kind": "function", "doc": "<p>@public</p>\n\n<p>Generate a rectangle on the given image with the specified color and fill.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>img : numpy.ndarray\n    The input image on which to draw the rectangle.\ncolor : tuple\n    A tuple representing the color of the rectangle in BGR format (e.g., (255, 0, 0) for blue).\nfill : bool\n    A boolean indicating whether the rectangle should be filled (True) or outlined (False).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>img : numpy.ndarray\n    The image with the rectangle drawn on it.\nbbox : tuple\n    A tuple containing two points that represent the bounding box of the rectangle\n    ((x1, y1), (x2, y2)).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">img</span>, </span><span class=\"param\"><span class=\"n\">color</span>, </span><span class=\"param\"><span class=\"n\">fill</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._gen_square", "modulename": "data_generator", "qualname": "DataGenerator._gen_square", "kind": "function", "doc": "<p>@public</p>\n\n<p>Generate a square on the given image with the specified color and fill.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>img : numpy.ndarray\n    The input image on which to draw the square.\ncolor : tuple\n    A tuple representing the color of the square in BGR format (e.g., (255, 0, 0) for blue).\nfill : bool\n    A boolean indicating whether the square should be filled (True) or outlined (False).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>img : numpy.ndarray\n    The image with the square drawn on it.\nbbox : tuple\n    A tuple containing two points that represent the bounding box of the square\n    ((x1, y1), (x2, y2)).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">img</span>, </span><span class=\"param\"><span class=\"n\">color</span>, </span><span class=\"param\"><span class=\"n\">fill</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._gen_circle", "modulename": "data_generator", "qualname": "DataGenerator._gen_circle", "kind": "function", "doc": "<p>@public</p>\n\n<p>Generate a circle on the given image with the specified color and fill.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>img : numpy.ndarray\n    The input image on which to draw the circle.\ncolor : tuple\n    A tuple representing the color of the circle in BGR format (e.g., (255, 0, 0) for blue).\nfill : bool\n    A boolean indicating whether the circle should be filled (True) or outlined (False).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>img : numpy.ndarray\n    The image with the circle drawn on it.\nbbox : tuple\n    A tuple containing two points that represent the bounding box of the circle\n    ((bb_x1, bb_y1), (bb_x2, bb_y2)).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">img</span>, </span><span class=\"param\"><span class=\"n\">color</span>, </span><span class=\"param\"><span class=\"n\">fill</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._gen_triangle", "modulename": "data_generator", "qualname": "DataGenerator._gen_triangle", "kind": "function", "doc": "<p>@public</p>\n\n<p>Generate a triangle on the given image with the specified color and fill.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>img : numpy.ndarray\n    The input image on which to draw the triangle.\ncolor : tuple\n    A tuple representing the color of the triangle in BGR format (e.g., (255, 0, 0) for blue).\nfill : bool\n    A boolean indicating whether the triangle should be filled (True) or outlined (False).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>img : numpy.ndarray\n    The image with the triangle drawn on it.\nbbox : tuple\n    A tuple containing two points that represent the bounding box of the triangle\n    ((min_x, min_y), (max_x, max_y)).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">img</span>, </span><span class=\"param\"><span class=\"n\">color</span>, </span><span class=\"param\"><span class=\"n\">fill</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._gen_pentagon", "modulename": "data_generator", "qualname": "DataGenerator._gen_pentagon", "kind": "function", "doc": "<p>@public</p>\n\n<p>Generate a pentagon on the given image with the specified color and fill.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>img : numpy.ndarray\n    The input image on which to draw the pentagon.\ncolor : tuple\n    A tuple representing the color of the pentagon in BGR format (e.g., (255, 0, 0) for blue).\nfill : bool\n    A boolean indicating whether the pentagon should be filled (True) or outlined (False).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>img : numpy.ndarray\n    The image with the pentagon drawn on it.\nbbox : tuple\n    A tuple containing two points that represent the bounding box of the pentagon\n    ((min_x, min_y), (max_x, max_y)).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">img</span>, </span><span class=\"param\"><span class=\"n\">color</span>, </span><span class=\"param\"><span class=\"n\">fill</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._generate_img", "modulename": "data_generator", "qualname": "DataGenerator._generate_img", "kind": "function", "doc": "<p>@public</p>\n\n<p>Generate an image with randomly placed shapes according to specified parameters.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>min_objects : int\n    Minimum number of objects to include in the image.\nmax_objects : int\n    Maximum number of objects to include in the image.\nvisualize_bboxes : bool\n    Whether to draw bounding boxes around the shapes.\nadd_noise : bool\n    Whether to add noise to the image.\nnoise_prob : float\n    The probability of noise presence in the image.\nfixed_colors : bool\n    Whether to use a fixed set of colors from the palette.\nfill_objects : bool\n    Whether the shapes should be filled (True) or outlined (False).\nshapes : dict\n    A dictionary of shape generation functions.\npalette : dict\n    A color palette for the shapes.\nreusable_palette_colors : bool\n    Whether colors from the palette can be reused.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>img : numpy.ndarray\n    The generated image with the specified number of shapes.\nobjects_info : list\n    A list of dictionaries containing information about each shape in the image.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">min_objects</span>,</span><span class=\"param\">\t<span class=\"n\">max_objects</span>,</span><span class=\"param\">\t<span class=\"n\">visualize_bboxes</span>,</span><span class=\"param\">\t<span class=\"n\">add_noise</span>,</span><span class=\"param\">\t<span class=\"n\">noise_prob</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_colors</span>,</span><span class=\"param\">\t<span class=\"n\">fill_objects</span>,</span><span class=\"param\">\t<span class=\"n\">shapes</span>,</span><span class=\"param\">\t<span class=\"n\">palette</span>,</span><span class=\"param\">\t<span class=\"n\">reusable_palette_colors</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._draw_bounding_box", "modulename": "data_generator", "qualname": "DataGenerator._draw_bounding_box", "kind": "function", "doc": "<p>@public</p>\n\n<p>Draw a bounding box on the given image.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>img : numpy.ndarray\n    The input image on which to draw the bounding box.\nbounding_box : tuple\n    A tuple containing two points that represent the bounding box\n    ((x1, y1), (x2, y2)).\ncolor : tuple, optional\n    A tuple representing the color of the bounding box in BGR format (default is white).\nthickness : int, optional\n    The thickness of the bounding box lines (default is 1).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>img : numpy.ndarray\n    The image with the bounding box drawn on it.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">img</span>, </span><span class=\"param\"><span class=\"n\">bounding_box</span>, </span><span class=\"param\"><span class=\"n\">color</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">255</span><span class=\"p\">,</span> <span class=\"mi\">255</span><span class=\"p\">,</span> <span class=\"mi\">255</span><span class=\"p\">)</span>, </span><span class=\"param\"><span class=\"n\">thickness</span><span class=\"o\">=</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._euclidean_distance", "modulename": "data_generator", "qualname": "DataGenerator._euclidean_distance", "kind": "function", "doc": "<p>@public</p>\n\n<p>Calculate the Euclidean distance between two points.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>point1 : tuple\n    A tuple representing the coordinates of the first point (x1, y1).\npoint2 : tuple\n    A tuple representing the coordinates of the second point (x2, y2).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float\n    The Euclidean distance between the two points, rounded to two decimal places.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">point1</span>, </span><span class=\"param\"><span class=\"n\">point2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._compute_bbox_center", "modulename": "data_generator", "qualname": "DataGenerator._compute_bbox_center", "kind": "function", "doc": "<p>@public</p>\n\n<p>Compute the center point of a bounding box.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>bbox : tuple\n    A tuple containing two points that represent the bounding box ((x1, y1), (x2, y2)).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>tuple\n    A tuple representing the coordinates of the center point of the bounding box (center_x, center_y).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">bbox</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._gen_noise", "modulename": "data_generator", "qualname": "DataGenerator._gen_noise", "kind": "function", "doc": "<p>@public</p>\n\n<p>Generate random noise on the given image based on a specified probability.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>img : numpy.ndarray\n    The input image on which to add noise.\nprob : float\n    The probability of each pixel being replaced by noise (value between 0 and 1).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>output : numpy.ndarray\n    The image with random noise added to it.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">img</span>, </span><span class=\"param\"><span class=\"n\">prob</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._find_line_endpoints", "modulename": "data_generator", "qualname": "DataGenerator._find_line_endpoints", "kind": "function", "doc": "<p>@public</p>\n\n<p>Find the endpoints of a line segment given its center, slope, and length.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>center_x : float\n    The x-coordinate of the center point of the line.\ncenter_y : float\n    The y-coordinate of the center point of the line.\nslope : float\n    The slope of the line.\nlength : float\n    The total length of the line segment.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>tuple\n    A tuple containing the start point (start_x, start_y) and end point (end_x, end_y) of the line.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">center_x</span>, </span><span class=\"param\"><span class=\"n\">center_y</span>, </span><span class=\"param\"><span class=\"n\">slope</span>, </span><span class=\"param\"><span class=\"n\">length</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._get_bounding_rect", "modulename": "data_generator", "qualname": "DataGenerator._get_bounding_rect", "kind": "function", "doc": "<p>@public</p>\n\n<p>Compute the bounding rectangle for a given set of points.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>points : list of tuples\n    A list of tuples, where each tuple represents the (x, y) coordinates of a point.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>tuple\n    A tuple containing two points that represent the top-left and bottom-right corners\n    of the bounding rectangle ((min_x, min_y), (max_x, max_y)).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">points</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._is_inside_image", "modulename": "data_generator", "qualname": "DataGenerator._is_inside_image", "kind": "function", "doc": "<p>@public</p>\n\n<p>Check if a set of points is fully inside the boundaries of an image.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>points : list of tuples\n    A list of tuples, where each tuple represents the (x, y) coordinates of a point.\nimg_dim : int\n    The dimension (width and height) of the image.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>bool\n    True if all points are inside the image boundaries, False otherwise.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">points</span>, </span><span class=\"param\"><span class=\"n\">img_dim</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._do_overlap", "modulename": "data_generator", "qualname": "DataGenerator._do_overlap", "kind": "function", "doc": "<p>@public</p>\n\n<p>Check if two bounding boxes overlap with an offset applied to all sides.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>box1 : tuple\n    A tuple containing two points that represent the first bounding box ((x1, y1), (x2, y2)).\nbox2 : tuple\n    A tuple containing two points that represent the second bounding box ((x1, y1), (x2, y2)).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>bool\n    True if the bounding boxes overlap, False otherwise.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">box1</span>, </span><span class=\"param\"><span class=\"n\">box2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._add_pairwise_distances", "modulename": "data_generator", "qualname": "DataGenerator._add_pairwise_distances", "kind": "function", "doc": "<p>@public</p>\n\n<p>Add pairwise distances between all objects in the image to their information.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>objects_info : list\n    A list of dictionaries, where each dictionary contains information about an object \n    in the image, including its bounding box location.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>list\n    The updated list of dictionaries with pairwise distances added for each object.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">objects_info</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._add_questions_and_answers", "modulename": "data_generator", "qualname": "DataGenerator._add_questions_and_answers", "kind": "function", "doc": "<p>@public</p>\n\n<p>Add relational and non-relational questions and answers to the image data.</p>\n\n<p>This method generates 10 relational and 10 non-relational questions based on the objects\nin the image and appends these questions, along with their vectors and answers, to the objects' information.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>img_dim : int\n    The dimension (width and height) of the image.\nobjects_info : list\n    A list of dictionaries, where each dictionary contains information about an object \n    in the image, including its bounding box location.\npalette : dict\n    A dictionary representing the color palette used for the objects.\nshapes : dict\n    A dictionary of shape generation functions.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>list\n    The updated list of dictionaries with questions and answers added for each image.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">img_dim</span>, </span><span class=\"param\"><span class=\"n\">objects_info</span>, </span><span class=\"param\"><span class=\"n\">palette</span>, </span><span class=\"param\"><span class=\"n\">shapes</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._get_random_question_vector", "modulename": "data_generator", "qualname": "DataGenerator._get_random_question_vector", "kind": "function", "doc": "<p>@public</p>\n\n<p>Generate a random question vector for relational or non-relational questions.</p>\n\n<p>As per the paper, the question vector is an 11-dimensional one-hot vector composed of:</p>\n\n<ul>\n<li>A 6-bit one-hot vector for the color.</li>\n<li>A 2-bit one-hot vector for the question subtype.</li>\n<li>A 3-bit one-hot vector for the specific question.</li>\n</ul>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>relational : bool, optional\n    Whether the question vector is for a relational question (True) or non-relational question (False).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>list\n    An 11-dimensional list representing the question vector.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">relational</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._get_question_subtype", "modulename": "data_generator", "qualname": "DataGenerator._get_question_subtype", "kind": "function", "doc": "<p>@public</p>\n\n<p>Decide the subtype of a question based on the given question vector.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>vector : list\n    An 11-dimensional list representing the question vector.\nrelational : bool, optional\n    Whether the question is relational (True) or non-relational (False).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>str\n    The subtype of the question. For relational questions, the subtype can be \"closest\", \n    \"furthest\", or \"count\". For non-relational questions, the subtype can be \"topbottom\", \n    \"leftright\", or \"shape\".</p>\n\n<h2 id=\"raises\">Raises:</h2>\n\n<p>ValueError\n    If the vector does not correspond to any known question subtype.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">vector</span>, </span><span class=\"param\"><span class=\"n\">relational</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._get_question_text", "modulename": "data_generator", "qualname": "DataGenerator._get_question_text", "kind": "function", "doc": "<p>@public</p>\n\n<p>Generate the text of a question based on its subtype and color.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>subtype : str\n    The subtype of the question (e.g., \"closest\", \"furthest\", \"count\" for relational questions, \n    or \"topbottom\", \"leftright\", \"shape\" for non-relational questions).\ncolor : str\n    The color of the object that is the subject of the question.\nrelational : bool, optional\n    Whether the question is relational (True) or non-relational (False).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>str\n    The text of the generated question.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">subtype</span>, </span><span class=\"param\"><span class=\"n\">color</span>, </span><span class=\"param\"><span class=\"n\">relational</span><span class=\"o\">=</span><span class=\"kc\">True</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._get_color_from_vector", "modulename": "data_generator", "qualname": "DataGenerator._get_color_from_vector", "kind": "function", "doc": "<p>@public</p>\n\n<p>Determine the color from the question vector using the given palette.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>vector : list\n    An 11-dimensional list representing the question vector.\npalette : dict\n    A dictionary representing the color palette, where keys are color names.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>str\n    The name of the color corresponding to the one-hot encoded vector.</p>\n\n<h2 id=\"raises\">Raises:</h2>\n\n<p>ValueError\n    If the vector does not correspond to any known color in the palette.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">vector</span>, </span><span class=\"param\"><span class=\"n\">palette</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._get_closest_object", "modulename": "data_generator", "qualname": "DataGenerator._get_closest_object", "kind": "function", "doc": "<p>@public</p>\n\n<p>Find the object that is closest to the specified color in the list of objects.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>color : str\n    The color of the reference object to find the closest object to.\nobjects_info : list\n    A list of dictionaries, where each dictionary contains information about an object \n    in the image, including its color and distances to other objects.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>dict or None\n    A dictionary containing information about the closest object to the specified color, \n    or None if no such object is found.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">color</span>, </span><span class=\"param\"><span class=\"n\">objects_info</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._get_furthest_object", "modulename": "data_generator", "qualname": "DataGenerator._get_furthest_object", "kind": "function", "doc": "<p>@public</p>\n\n<p>Find the object that is furthest from the specified color in the list of objects.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>color : str\n    The color of the reference object to find the furthest object from.\nobjects_info : list\n    A list of dictionaries, where each dictionary contains information about an object \n    in the image, including its color and distances to other objects.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>dict or None\n    A dictionary containing information about the furthest object from the specified color, \n    or None if no such object is found.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">color</span>, </span><span class=\"param\"><span class=\"n\">objects_info</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._get_count_of_objects_with_shape", "modulename": "data_generator", "qualname": "DataGenerator._get_count_of_objects_with_shape", "kind": "function", "doc": "<p>@public</p>\n\n<p>Count the number of objects with the same shape as the object of the specified color.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>color : str\n    The color of the reference object to find the shape of.\nobjects_info : list\n    A list of dictionaries, where each dictionary contains information about an object \n    in the image, including its color and shape.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>int\n    The count of objects that have the same shape as the object of the specified color.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">color</span>, </span><span class=\"param\"><span class=\"n\">objects_info</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._get_relational_answer", "modulename": "data_generator", "qualname": "DataGenerator._get_relational_answer", "kind": "function", "doc": "<p>@public</p>\n\n<p>Determine the answer to a relational question based on its subtype and reference color.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>subtype : str\n    The subtype of the question (e.g., \"closest\", \"furthest\", \"count\").\ncolor : str\n    The color of the reference object for the question.\nobjects_info : list\n    A list of dictionaries, where each dictionary contains information about an object \n    in the image, including its color, shape, and distances to other objects.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>str or int\n    The answer to the relational question. This could be a color name, object type, or a count \n    depending on the subtype of the question.</p>\n\n<h2 id=\"raises\">Raises:</h2>\n\n<p>ValueError\n    If the subtype does not correspond to any known question type.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">subtype</span>, </span><span class=\"param\"><span class=\"n\">color</span>, </span><span class=\"param\"><span class=\"n\">objects_info</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._get_non_relational_answer", "modulename": "data_generator", "qualname": "DataGenerator._get_non_relational_answer", "kind": "function", "doc": "<p>@public</p>\n\n<p>Determine the answer to a non-relational question based on its subtype and reference color.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>img_dim : int\n    The dimension (width and height) of the image.\nsubtype : str\n    The subtype of the question (e.g., \"topbottom\", \"leftright\", \"shape\").\ncolor : str\n    The color of the reference object for the question.\nobjects_info : list\n    A list of dictionaries, where each dictionary contains information about an object \n    in the image, including its color, shape, and bounding box location.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>str\n    The answer to the non-relational question. This could be \"top\", \"bottom\", \"left\", \"right\",\n    or the shape of the object, depending on the subtype of the question.</p>\n\n<h2 id=\"raises\">Raises:</h2>\n\n<p>ValueError\n    If the subtype does not correspond to any known question type.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">img_dim</span>, </span><span class=\"param\"><span class=\"n\">subtype</span>, </span><span class=\"param\"><span class=\"n\">color</span>, </span><span class=\"param\"><span class=\"n\">objects_info</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator._add_image_description_matrix", "modulename": "data_generator", "qualname": "DataGenerator._add_image_description_matrix", "kind": "function", "doc": "<p>@public</p>\n\n<p>Add an image description matrix to the objects' information.</p>\n\n<p>The matrix has a shape of (num_objects, num_attributes), where num_attributes = 6, \nrepresenting the color, shape, and 4 bounding box coordinates for each object.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>objects_info : list\n    A list of dictionaries, where each dictionary contains information about an object \n    in the image, including its color, shape, and bounding box location.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>list\n    The updated list of dictionaries with the image description matrix added for each object.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">objects_info</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.DataGenerator.generate_dataset", "modulename": "data_generator", "qualname": "DataGenerator.generate_dataset", "kind": "function", "doc": "<p>Generate a dataset of images with various geometric shapes and save the details in a JSON file.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>img_dim : int, optional\n    The dimension (width and height) of each generated image (default is 128).\nnum_images : int, optional\n    The number of images to generate (default is 100).\nmin_objects : int, optional\n    Minimum number of objects to include in each image (default is 0).\nmax_objects : int, optional\n    Maximum number of objects to include in each image (default is 6).\nvisualize_bboxes : bool, optional\n    Whether to draw bounding boxes around the shapes (default is True).\nadd_noise : bool, optional\n    Whether to add noise to the images (default is False).\nnoise_prob : float, optional\n    The probability of noise presence in the images (default is 0.05).\nfixed_colors : bool, optional\n    Whether to use a fixed set of colors from the palette (default is True).\nfill_objects : bool, optional\n    Whether the shapes should be filled (default is False).\nreusable_palette_colors : bool, optional\n    Whether colors from the palette can be reused (default is False).\nshapes : dict, optional\n    A dictionary of shape generation functions (default is a set of predefined shapes).\npalette : dict, optional\n    A color palette for the shapes (default is a set of predefined colors).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>None</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">img_dim</span><span class=\"o\">=</span><span class=\"mi\">128</span>,</span><span class=\"param\">\t<span class=\"n\">num_images</span><span class=\"o\">=</span><span class=\"mi\">100</span>,</span><span class=\"param\">\t<span class=\"n\">min_objects</span><span class=\"o\">=</span><span class=\"mi\">6</span>,</span><span class=\"param\">\t<span class=\"n\">max_objects</span><span class=\"o\">=</span><span class=\"mi\">6</span>,</span><span class=\"param\">\t<span class=\"n\">visualize_bboxes</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">add_noise</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">noise_prob</span><span class=\"o\">=</span><span class=\"mf\">0.05</span>,</span><span class=\"param\">\t<span class=\"n\">fixed_colors</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">fill_objects</span><span class=\"o\">=</span><span class=\"kc\">True</span>,</span><span class=\"param\">\t<span class=\"n\">reusable_palette_colors</span><span class=\"o\">=</span><span class=\"kc\">False</span>,</span><span class=\"param\">\t<span class=\"n\">shapes</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">palette</span><span class=\"o\">=</span><span class=\"kc\">None</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "data_generator.NumpyEncoder", "modulename": "data_generator", "qualname": "NumpyEncoder", "kind": "class", "doc": "<p>Extensible JSON <a href=\"https://json.org\">https://json.org</a> encoder for Python data structures.</p>\n\n<p>Supports the following objects and types by default:</p>\n\n<p>+-------------------+---------------+\n| Python            | JSON          |\n+===================+===============+\n| dict              | object        |\n+-------------------+---------------+\n| list, tuple       | array         |\n+-------------------+---------------+\n| str               | string        |\n+-------------------+---------------+\n| int, float        | number        |\n+-------------------+---------------+\n| True              | true          |\n+-------------------+---------------+\n| False             | false         |\n+-------------------+---------------+\n| None              | null          |\n+-------------------+---------------+</p>\n\n<p>To extend this to recognize other objects, subclass and implement a\n<code>.default()</code> method with another method that returns a serializable\nobject for <code>o</code> if possible, otherwise it should call the superclass\nimplementation (to raise <code>TypeError</code>).</p>\n", "bases": "json.encoder.JSONEncoder"}, {"fullname": "data_generator.NumpyEncoder.default", "modulename": "data_generator", "qualname": "NumpyEncoder.default", "kind": "function", "doc": "<p>Implement this method in a subclass such that it returns\na serializable object for <code>o</code>, or calls the base implementation\n(to raise a <code>TypeError</code>).</p>\n\n<p>For example, to support arbitrary iterators, you could\nimplement default like this::</p>\n\n<pre><code>def default(self, o):\n    try:\n        iterable = iter(o)\n    except TypeError:\n        pass\n    else:\n        return list(iterable)\n    # Let the base class default method raise the TypeError\n    return JSONEncoder.default(self, o)\n</code></pre>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">obj</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "dataset_builder", "modulename": "dataset_builder", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.DatasetBuilder", "modulename": "dataset_builder", "qualname": "DatasetBuilder", "kind": "class", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.DatasetBuilder.__init__", "modulename": "dataset_builder", "qualname": "DatasetBuilder.__init__", "kind": "function", "doc": "<p>Initializes the DatasetBuilder by loading, splitting, and processing the dataset.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>data_dir : str\n    The directory containing the images and dataset JSON file.\ntransform : torchvision.transforms.Compose\n    Transformations to apply to the images.\ntransform_prob : float\n    The probability of applying transformations (default=0).\nrandom_seed : int\n    Random seed for reproducibility (default=42).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">data_dir</span>, </span><span class=\"param\"><span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">transform_prob</span><span class=\"o\">=</span><span class=\"mi\">0</span>, </span><span class=\"param\"><span class=\"n\">random_seed</span><span class=\"o\">=</span><span class=\"mi\">42</span></span>)</span>"}, {"fullname": "dataset_builder.DatasetBuilder.data_dir", "modulename": "dataset_builder", "qualname": "DatasetBuilder.data_dir", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.DatasetBuilder.transform", "modulename": "dataset_builder", "qualname": "DatasetBuilder.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.DatasetBuilder.transform_prob", "modulename": "dataset_builder", "qualname": "DatasetBuilder.transform_prob", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.DatasetBuilder.random_seed", "modulename": "dataset_builder", "qualname": "DatasetBuilder.random_seed", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.DatasetBuilder.vocab", "modulename": "dataset_builder", "qualname": "DatasetBuilder.vocab", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.DatasetBuilder.answer_vocab", "modulename": "dataset_builder", "qualname": "DatasetBuilder.answer_vocab", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.DatasetBuilder.max_len", "modulename": "dataset_builder", "qualname": "DatasetBuilder.max_len", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.DatasetBuilder.train_dataset", "modulename": "dataset_builder", "qualname": "DatasetBuilder.train_dataset", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.DatasetBuilder.val_dataset", "modulename": "dataset_builder", "qualname": "DatasetBuilder.val_dataset", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.DatasetBuilder.test_dataset", "modulename": "dataset_builder", "qualname": "DatasetBuilder.test_dataset", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.DatasetBuilder._load_data", "modulename": "dataset_builder", "qualname": "DatasetBuilder._load_data", "kind": "function", "doc": "<p>@public</p>\n\n<p>Loads the dataset from the 'descr.json' file and splits it into train, validation, and test sets.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>train_size : float\n    Proportion of the data to be used for training.\nval_size : float\n    Proportion of the data to be used for validation.\ntest_size : float\n    Proportion of the data to be used for testing.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">train_size</span><span class=\"o\">=</span><span class=\"mf\">0.7</span>, </span><span class=\"param\"><span class=\"n\">val_size</span><span class=\"o\">=</span><span class=\"mf\">0.1</span>, </span><span class=\"param\"><span class=\"n\">test_size</span><span class=\"o\">=</span><span class=\"mf\">0.2</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "dataset_builder.DatasetBuilder._generate_samples", "modulename": "dataset_builder", "qualname": "DatasetBuilder._generate_samples", "kind": "function", "doc": "<p>@public</p>\n\n<p>Generates samples for the dataset (20 per image: 10 relational, 10 non-relational).</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>data : dict\n    The loaded dataset dictionary.\nimage_paths : list\n    A list of image paths corresponding to a specific split.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>list\n    A list of samples where each sample is a tuple (image_path, question, answer).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">data</span>, </span><span class=\"param\"><span class=\"n\">image_paths</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "dataset_builder.DatasetBuilder._compute_max_question_len", "modulename": "dataset_builder", "qualname": "DatasetBuilder._compute_max_question_len", "kind": "function", "doc": "<p>@public</p>\n\n<p>Computes the maximum length of questions in the dataset.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>samples : list\n    A list of samples where each sample contains a question.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>int\n    The length of the longest question in the dataset.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">samples</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "dataset_builder.DatasetBuilder._build_vocab", "modulename": "dataset_builder", "qualname": "DatasetBuilder._build_vocab", "kind": "function", "doc": "<p>@public</p>\n\n<p>Builds a vocabulary from the dataset questions and assigns an index to each unique word.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>samples : list\n    A list of samples where each sample contains a question.\nmin_freq : int\n    Minimum frequency for a word to be included in the vocabulary (default=1).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>dict\n    A dictionary mapping each word to a unique index.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">samples</span>, </span><span class=\"param\"><span class=\"n\">min_freq</span><span class=\"o\">=</span><span class=\"mi\">1</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "dataset_builder.DatasetBuilder._build_answer_vocab", "modulename": "dataset_builder", "qualname": "DatasetBuilder._build_answer_vocab", "kind": "function", "doc": "<p>@public</p>\n\n<p>Builds a vocabulary from the dataset answers, mapping each answer to a unique index.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>samples : list\n    A list of samples where each sample contains an answer.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>dict\n    A dictionary mapping each answer to a unique index.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">samples</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "dataset_builder.RelationalDataset", "modulename": "dataset_builder", "qualname": "RelationalDataset", "kind": "class", "doc": "<p>A PyTorch Dataset class to load relational reasoning data, including both relational \nand non-relational questions. Supports data augmentation and train/val/test splitting.</p>\n\n<h2 id=\"attributes\">Attributes:</h2>\n\n<p>samples : list\n    The dataset samples with image paths, questions, and answers.\nvocab : dict\n    A dictionary mapping question words to token indices.\nanswer_vocab : dict\n    A dictionary mapping answers to token indices.\nmax_len : int\n    The maximum length of tokenized questions for padding.\ntransform : ImageAnswerTransform\n    Data transformations that apply both image and text transformations.\ntransform_prob : float\n    The probability of applying transformations.<br />\nrandom_seed : int\n    Random seed for reproducibility and for use in transformations.</p>\n", "bases": "typing.Generic[+T_co]"}, {"fullname": "dataset_builder.RelationalDataset.__init__", "modulename": "dataset_builder", "qualname": "RelationalDataset.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">samples</span>,</span><span class=\"param\">\t<span class=\"n\">vocab</span>,</span><span class=\"param\">\t<span class=\"n\">answer_vocab</span>,</span><span class=\"param\">\t<span class=\"n\">max_len</span>,</span><span class=\"param\">\t<span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>,</span><span class=\"param\">\t<span class=\"n\">transform_prob</span><span class=\"o\">=</span><span class=\"mi\">0</span>,</span><span class=\"param\">\t<span class=\"n\">random_seed</span><span class=\"o\">=</span><span class=\"mi\">42</span></span>)</span>"}, {"fullname": "dataset_builder.RelationalDataset.samples", "modulename": "dataset_builder", "qualname": "RelationalDataset.samples", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.RelationalDataset.vocab", "modulename": "dataset_builder", "qualname": "RelationalDataset.vocab", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.RelationalDataset.answer_vocab", "modulename": "dataset_builder", "qualname": "RelationalDataset.answer_vocab", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.RelationalDataset.max_len", "modulename": "dataset_builder", "qualname": "RelationalDataset.max_len", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.RelationalDataset.transform", "modulename": "dataset_builder", "qualname": "RelationalDataset.transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.RelationalDataset.transform_prob", "modulename": "dataset_builder", "qualname": "RelationalDataset.transform_prob", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.RelationalDataset.random_seed", "modulename": "dataset_builder", "qualname": "RelationalDataset.random_seed", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.RelationalDataset._tokenize_question", "modulename": "dataset_builder", "qualname": "RelationalDataset._tokenize_question", "kind": "function", "doc": "<p>@public</p>\n\n<p>Tokenizes the question into a list of token indices and pads or truncates to max_len.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>question : str\n    The question to tokenize.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>list\n    A list of token indices.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">question</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "dataset_builder.RelationalDataset._encode_answer", "modulename": "dataset_builder", "qualname": "RelationalDataset._encode_answer", "kind": "function", "doc": "<p>@public</p>\n\n<p>Converts an answer into its corresponding index.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>answer : str\n    The answer to encode.\nanswer_vocab : dict\n    A dictionary mapping answers to indices.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>int\n    The index of the answer in the answer vocabulary.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">answer</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "dataset_builder.ImageAnswerTransform", "modulename": "dataset_builder", "qualname": "ImageAnswerTransform", "kind": "class", "doc": "<p>A class that applies both image and text transformations, allowing \nthem to communicate via a shared state. The state can be used to \npass transformation parameters or results from the image transformation \nto the text transformation, allowing for coordinated changes between \nthe image and the label.</p>\n\n<h2 id=\"attributes\">Attributes:</h2>\n\n<p>image_transform : callable, optional\n    A transformation function for the image. It should accept a PIL Image \n    and a state dictionary, then return the transformed PIL Image and the updated state.</p>\n\n<p>text_transform : callable, optional\n    A transformation function for the label (string). It should accept a string label \n    and the shared state, then return the transformed label.</p>\n\n<p>state : dict, optional\n    A shared state dictionary used to pass information between the image \n    and text transformations. If not provided, it is initialized as an empty dictionary.</p>\n\n<h2 id=\"methods\">Methods:</h2>\n\n<p>__call__(img, label)\n    Applies the image transformation, updates the shared state, and then \n    applies the text transformation using the updated state.</p>\n\n<pre><code>Parameters:\n-----------\nimg : PIL.Image\n    The input image to be transformed.\n\nlabel : str\n    The label associated with the image, which may be transformed \n    based on the shared state.\n\nReturns:\n--------\nimg : PIL.Image\n    The transformed image.\n\nlabel : str\n    The transformed label, modified based on the image transformation \n    and the state.\n</code></pre>\n"}, {"fullname": "dataset_builder.ImageAnswerTransform.__init__", "modulename": "dataset_builder", "qualname": "ImageAnswerTransform.__init__", "kind": "function", "doc": "<p></p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">image_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">text_transform</span><span class=\"o\">=</span><span class=\"kc\">None</span>, </span><span class=\"param\"><span class=\"n\">state</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "dataset_builder.ImageAnswerTransform.image_transform", "modulename": "dataset_builder", "qualname": "ImageAnswerTransform.image_transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.ImageAnswerTransform.text_transform", "modulename": "dataset_builder", "qualname": "ImageAnswerTransform.text_transform", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.ImageAnswerTransform.state", "modulename": "dataset_builder", "qualname": "ImageAnswerTransform.state", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "dataset_builder.Rotate180DegreesTransform", "modulename": "dataset_builder", "qualname": "Rotate180DegreesTransform", "kind": "class", "doc": "<p>A specialized ImageAnswerTransform that rotates the image by 180 degrees and modifies the answer accordingly.</p>\n\n<p>This class inherits from ImageAnswerTransform and implements the specific functionality of rotating an image\nby 180 degrees and transforming the directional answers ('top', 'bottom', 'left', 'right') based on the rotation.</p>\n\n<h2 id=\"methods\">Methods:</h2>\n\n<p>rotate_image_180(img, state):\n    Rotates the image by 180 degrees clockwise.\nmodify_answer_based_on_rotation(answer, state):\n    Modifies the answer according to the 180-degree rotation.</p>\n", "bases": "ImageAnswerTransform"}, {"fullname": "dataset_builder.Rotate180DegreesTransform.__init__", "modulename": "dataset_builder", "qualname": "Rotate180DegreesTransform.__init__", "kind": "function", "doc": "<p>Initializes the Rotate180DegreesTransform with predefined image and text transformations.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>state : dict, optional\n    A shared state dictionary used to pass information between the image and text transformations.\n    If not provided, it is initialized as an empty dictionary.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">state</span><span class=\"o\">=</span><span class=\"kc\">None</span></span>)</span>"}, {"fullname": "dataset_builder.Rotate180DegreesTransform.rotate_image_180", "modulename": "dataset_builder", "qualname": "Rotate180DegreesTransform.rotate_image_180", "kind": "function", "doc": "<p>Rotate the image by 180 degrees clockwise.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>img : PIL.Image\n    The image to be rotated.\nstate : dict\n    The state used for shared information between image and text transformations.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>img : PIL.Image\n    The rotated image.\nstate : dict\n    The unchanged state dictionary.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">img</span>, </span><span class=\"param\"><span class=\"n\">state</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "dataset_builder.Rotate180DegreesTransform.modify_answer_based_on_rotation", "modulename": "dataset_builder", "qualname": "Rotate180DegreesTransform.modify_answer_based_on_rotation", "kind": "function", "doc": "<p>Modify the answer based on the 180-degree rotation, assuming directional answers ('top', 'bottom', 'left', 'right').</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>answer : str\n    The original answer related to the image.\nstate : dict\n    The state used for shared information between image and text transformations.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>str\n    The modified answer after applying the 180-degree rotation.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">answer</span>, </span><span class=\"param\"><span class=\"n\">state</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "evaluator", "modulename": "evaluator", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "models", "modulename": "models", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "models.ImageEncoder", "modulename": "models", "qualname": "ImageEncoder", "kind": "class", "doc": "<p>ImageEncoder class using a pre-trained ResNet18 model for feature extraction,\nwith built-in image preprocessing.</p>\n\n<h2 id=\"methods\">Methods:</h2>\n\n<p>preprocess_image(image: torch.Tensor) -> torch.Tensor\n    Applies preprocessing to an input image.\nforward(image: torch.Tensor) -> torch.Tensor\n    Passes the preprocessed input image through the CNN to extract feature maps.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "models.ImageEncoder.__init__", "modulename": "models", "qualname": "ImageEncoder.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">()</span>"}, {"fullname": "models.ImageEncoder.cnn", "modulename": "models", "qualname": "ImageEncoder.cnn", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "models.ImageEncoder.preprocess", "modulename": "models", "qualname": "ImageEncoder.preprocess", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "models.ImageEncoder.preprocess_image", "modulename": "models", "qualname": "ImageEncoder.preprocess_image", "kind": "function", "doc": "<p>Preprocesses the input image by resizing and normalizing.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>image : torch.Tensor\n    Input image tensor of shape (C, H, W).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>torch.Tensor\n    Preprocessed image tensor of shape (1, 3, 224, 224), with a batch dimension.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "models.ImageEncoder.forward", "modulename": "models", "qualname": "ImageEncoder.forward", "kind": "function", "doc": "<p>Forward pass to extract features from the input image.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>image : torch.Tensor\n    Input image in tensor format.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>torch.Tensor\n    Output tensor of shape (batch_size, 512, H, W), where 512 is the number of feature\n    channels, and H and W are the spatial dimensions of the output feature map.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "models.QuestionEncoder", "modulename": "models", "qualname": "QuestionEncoder", "kind": "class", "doc": "<p>Module for encoding questions using an LSTM.</p>\n\n<h2 id=\"methods\">Methods:</h2>\n\n<p>forward(questions):\n    Performs a forward pass through the network, returning the final hidden state of the LSTM.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "models.QuestionEncoder.__init__", "modulename": "models", "qualname": "QuestionEncoder.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">vocab_size</span>, </span><span class=\"param\"><span class=\"n\">embed_size</span>, </span><span class=\"param\"><span class=\"n\">hidden_size</span>, </span><span class=\"param\"><span class=\"n\">num_layers</span></span>)</span>"}, {"fullname": "models.QuestionEncoder.embedding", "modulename": "models", "qualname": "QuestionEncoder.embedding", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "models.QuestionEncoder.lstm", "modulename": "models", "qualname": "QuestionEncoder.lstm", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "models.QuestionEncoder.forward", "modulename": "models", "qualname": "QuestionEncoder.forward", "kind": "function", "doc": "<p>Performs a forward pass through the network.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>questions : torch.Tensor\n    A batch of tokenized questions represented as tensors.</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>torch.Tensor\n    The last hidden state of the LSTM for each question in the batch, with shape (batch_size, hidden_size).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">questions</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "models.RelationalNetwork", "modulename": "models", "qualname": "RelationalNetwork", "kind": "class", "doc": "<p>Module for relational reasoning, composed of two MLPs (g_theta and f_phi).</p>\n\n<h2 id=\"attributes\">Attributes</h2>\n\n<p>g_theta : nn.Sequential\n    An MLP that takes concatenated object features and question embeddings to learn relations.\nf_phi : nn.Sequential\n    A final MLP that takes the summed output of all g_theta outputs to produce a final prediction.</p>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>forward(object_features, question_embedding):\n    Performs a forward pass through the network using object features and a question embeddings.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "models.RelationalNetwork.__init__", "modulename": "models", "qualname": "RelationalNetwork.__init__", "kind": "function", "doc": "<p>Initializes the RelationalNetwork with two MLPs: g_theta and f_phi.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>feature_dim : int\n    The dimensionality of the object features.\nquestion_dim : int\n    The dimensionality of the question embedding.\ng_theta_dim : int\n    The dimensionality of the hidden layer in the g_theta MLP.\nf_phi_dim : int\n    The dimensionality of the hidden layer in the f_phi MLP.\nnum_classes : int\n    The number of output classes for the multi-class classification.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">feature_dim</span>, </span><span class=\"param\"><span class=\"n\">question_dim</span>, </span><span class=\"param\"><span class=\"n\">g_theta_dim</span>, </span><span class=\"param\"><span class=\"n\">f_phi_dim</span>, </span><span class=\"param\"><span class=\"n\">num_classes</span></span>)</span>"}, {"fullname": "models.RelationalNetwork.g_theta", "modulename": "models", "qualname": "RelationalNetwork.g_theta", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "models.RelationalNetwork.f_phi", "modulename": "models", "qualname": "RelationalNetwork.f_phi", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "models.RelationalNetwork.forward", "modulename": "models", "qualname": "RelationalNetwork.forward", "kind": "function", "doc": "<p>Performs a forward pass through the network.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>object_features : torch.Tensor\n    A tensor containing the features of objects, with shape (batch_size, num_objects, feature_dim).\nquestion_embedding : torch.Tensor\n    A tensor representing the question embedding, with shape (batch_size, question_dim).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>torch.Tensor\n    The output tensor, which contains the prediction result. The shape is (batch_size, num_classes).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">object_features</span>, </span><span class=\"param\"><span class=\"n\">question_embedding</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "models.RelationalReasoningModel", "modulename": "models", "qualname": "RelationalReasoningModel", "kind": "class", "doc": "<p>Model for visual question answering using relational reasoning.</p>\n\n<p>This model consists of three main components:</p>\n\n<ul>\n<li>ImageEncoder: encodes the input image into object features</li>\n<li>QuestionEncoder: encodes the input question into a vector embedding</li>\n<li>RelationalNetwork: performs relational reasoning over object features and the question embedding</li>\n</ul>\n\n<h2 id=\"methods\">Methods</h2>\n\n<p>forward(image, question):\n    Performs a forward pass through the network, processing the image and question to produce an output.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "models.RelationalReasoningModel.__init__", "modulename": "models", "qualname": "RelationalReasoningModel.__init__", "kind": "function", "doc": "<p>Initializes the RelationalReasoningModel with an ImageEncoder, QuestionEncoder, and RelationalNetwork.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>vocab_size : int\n    The size of the vocabulary used in the QuestionEncoder.\nembed_size : int\n    The dimensionality of the word embeddings in the QuestionEncoder.\nhidden_size : int\n    The number of features in the hidden state of the LSTM in the QuestionEncoder.\nnum_layers : int\n    The number of recurrent layers in the LSTM of the QuestionEncoder.\nfeature_dim : int\n    The dimensionality of the object features produced by the ImageEncoder.\ng_theta_dim : int\n    The dimensionality of the hidden layers in the g_theta MLP of the RelationalNetwork.\nf_phi_dim : int\n    The dimensionality of the hidden layers in the f_phi MLP of the RelationalNetwork.\nnum_classes : int\n    The number of output classes for the multi-class classification.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"n\">vocab_size</span>,</span><span class=\"param\">\t<span class=\"n\">embed_size</span>,</span><span class=\"param\">\t<span class=\"n\">hidden_size</span>,</span><span class=\"param\">\t<span class=\"n\">num_layers</span>,</span><span class=\"param\">\t<span class=\"n\">feature_dim</span>,</span><span class=\"param\">\t<span class=\"n\">g_theta_dim</span>,</span><span class=\"param\">\t<span class=\"n\">f_phi_dim</span>,</span><span class=\"param\">\t<span class=\"n\">num_classes</span></span>)</span>"}, {"fullname": "models.RelationalReasoningModel.image_encoder", "modulename": "models", "qualname": "RelationalReasoningModel.image_encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "models.RelationalReasoningModel.question_encoder", "modulename": "models", "qualname": "RelationalReasoningModel.question_encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "models.RelationalReasoningModel.relation_network", "modulename": "models", "qualname": "RelationalReasoningModel.relation_network", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "models.RelationalReasoningModel.forward", "modulename": "models", "qualname": "RelationalReasoningModel.forward", "kind": "function", "doc": "<p>Performs a forward pass through the model.</p>\n\n<h2 id=\"parameters\">Parameters</h2>\n\n<p>image : torch.Tensor\n    A tensor representing the input image of shape (batch_size, channels, height, width).\nquestion : torch.Tensor\n    A tensor representing the input question of shape (batch_size, seq_len).</p>\n\n<h2 id=\"returns\">Returns</h2>\n\n<p>torch.Tensor\n    The output tensor, representing the predicted class logits, with shape (batch_size, num_classes).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">image</span>, </span><span class=\"param\"><span class=\"n\">question</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "models.BaselineModel", "modulename": "models", "qualname": "BaselineModel", "kind": "class", "doc": "<p>Base model that combines a CNN-based image encoder and an LSTM-based question encoder\nwith a simple MLP for classification without a relational network module.</p>\n", "bases": "torch.nn.modules.module.Module"}, {"fullname": "models.BaselineModel.__init__", "modulename": "models", "qualname": "BaselineModel.__init__", "kind": "function", "doc": "<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">vocab_size</span>, </span><span class=\"param\"><span class=\"n\">embed_size</span>, </span><span class=\"param\"><span class=\"n\">hidden_size</span>, </span><span class=\"param\"><span class=\"n\">num_layers</span>, </span><span class=\"param\"><span class=\"n\">num_classes</span></span>)</span>"}, {"fullname": "models.BaselineModel.image_encoder", "modulename": "models", "qualname": "BaselineModel.image_encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "models.BaselineModel.question_encoder", "modulename": "models", "qualname": "BaselineModel.question_encoder", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "models.BaselineModel.fc1", "modulename": "models", "qualname": "BaselineModel.fc1", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "models.BaselineModel.fc2", "modulename": "models", "qualname": "BaselineModel.fc2", "kind": "variable", "doc": "<p></p>\n"}, {"fullname": "models.BaselineModel.forward", "modulename": "models", "qualname": "BaselineModel.forward", "kind": "function", "doc": "<p>Forward pass to get the model's prediction.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>image : torch.Tensor\n    Input image in tensor format.\nquestions : torch.Tensor\n    Input tensor containing word indices of shape (batch_size, seq_len).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>torch.Tensor\n    Output prediction tensor of shape (batch_size, num_classes).</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">image</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>, </span><span class=\"param\"><span class=\"n\">questions</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">Tensor</span>:</span></span>", "funcdef": "def"}, {"fullname": "models.ModelConstructor", "modulename": "models", "qualname": "ModelConstructor", "kind": "class", "doc": "<p>A class to manage and load different types of models: predefined models (BaselineModel, RelationalReasoningModel) \nor custom models in ONNX or PyTorch format, please mind the ONNX can be used only for inference.</p>\n\n<h2 id=\"methods\">Methods:</h2>\n\n<p>load_model(model_type: str, **kwargs) -> nn.Module:\n    Loads a predefined model ('baseline', 'relational') or a custom ONNX/PyTorch model based on the input parameters.</p>\n"}, {"fullname": "models.ModelConstructor.load_model", "modulename": "models", "qualname": "ModelConstructor.load_model", "kind": "function", "doc": "<p>Load and initialize one of the predefined models or a custom model.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>model_type : str\n    The type of model to load ('baseline', 'relational', 'onnx', or 'custom').\n**kwargs : dict\n    Additional arguments required to initialize the model (e.g., vocab_size, embed_size, etc.).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>nn.Module or ort.InferenceSession\n    The initialized PyTorch model or ONNX session for inference.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">model_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span>, </span><span class=\"param\"><span class=\"o\">**</span><span class=\"n\">kwargs</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>:</span></span>", "funcdef": "def"}, {"fullname": "models.ModelConstructor._load_custom_model", "modulename": "models", "qualname": "ModelConstructor._load_custom_model", "kind": "function", "doc": "<p>@public</p>\n\n<p>Loads a custom PyTorch model and optionally loads pre-trained weights.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>model_class : torch.nn.Module\n    The class or instance of the custom model to load.\nweights_path : str, optional\n    Path to the PyTorch model weights file (.pt or .pth) (default=None).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>nn.Module\n    The initialized PyTorch model, optionally with loaded weights.</p>\n", "signature": "<span class=\"signature pdoc-code multiline\">(<span class=\"param\">\t<span class=\"bp\">self</span>,</span><span class=\"param\">\t<span class=\"n\">model_class</span><span class=\"p\">:</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>,</span><span class=\"param\">\t<span class=\"n\">weights_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"kc\">None</span></span><span class=\"return-annotation\">) -> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">modules</span><span class=\"o\">.</span><span class=\"n\">module</span><span class=\"o\">.</span><span class=\"n\">Module</span>:</span></span>", "funcdef": "def"}, {"fullname": "models.ModelConstructor._load_custom_model_onnx", "modulename": "models", "qualname": "ModelConstructor._load_custom_model_onnx", "kind": "function", "doc": "<p>@public</p>\n\n<p>Loads a custom model from an ONNX file using ONNX Runtime.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>onnx_path : str\n    Path to the ONNX model file.</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>ort.InferenceSession\n    An ONNX Runtime session that can be used to make predictions with the ONNX model.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"bp\">self</span>, </span><span class=\"param\"><span class=\"n\">onnx_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "report_generator", "modulename": "report_generator", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "training", "modulename": "training", "kind": "module", "doc": "<p></p>\n"}, {"fullname": "training.train_one_epoch", "modulename": "training", "qualname": "train_one_epoch", "kind": "function", "doc": "<p>Train the model for one epoch.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>model : torch.nn.Module\n    The model to be trained.\ntrain_loader : DataLoader\n    DataLoader for the training dataset.\ncriterion : torch.nn.Module\n    The loss function.\noptimizer : torch.optim.Optimizer\n    The optimizer for model parameters.\ndevice : torch.device\n    Device where the model is placed (CPU or CUDA).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float\n    Average loss for the epoch.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">train_loader</span>, </span><span class=\"param\"><span class=\"n\">criterion</span>, </span><span class=\"param\"><span class=\"n\">optimizer</span>, </span><span class=\"param\"><span class=\"n\">device</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}, {"fullname": "training.validate_one_epoch", "modulename": "training", "qualname": "validate_one_epoch", "kind": "function", "doc": "<p>Validate the model on the validation dataset.</p>\n\n<h2 id=\"parameters\">Parameters:</h2>\n\n<p>model : torch.nn.Module\n    The model to be validated.\nval_loader : DataLoader\n    DataLoader for the validation dataset.\ncriterion : torch.nn.Module\n    The loss function.\ndevice : torch.device\n    Device where the model is placed (CPU or CUDA).</p>\n\n<h2 id=\"returns\">Returns:</h2>\n\n<p>float\n    Average validation loss for the epoch.\nfloat\n    Accuracy on the validation dataset.</p>\n", "signature": "<span class=\"signature pdoc-code condensed\">(<span class=\"param\"><span class=\"n\">model</span>, </span><span class=\"param\"><span class=\"n\">val_loader</span>, </span><span class=\"param\"><span class=\"n\">criterion</span>, </span><span class=\"param\"><span class=\"n\">device</span></span><span class=\"return-annotation\">):</span></span>", "funcdef": "def"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();